{"cells":[{"cell_type":"code","execution_count":10,"metadata":{"id":"Zk9HGMcHw79S","executionInfo":{"status":"ok","timestamp":1681396814607,"user_tz":300,"elapsed":5,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# import torchvision.transforms as transforms\n","# import torchvision.models as models\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"SQVC-aHcjEvN","executionInfo":{"status":"ok","timestamp":1681396816557,"user_tz":300,"elapsed":4,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["\n","class ConvEncoder(nn.Module):\n","  \"\"\" create convolutional layers to extract features\n","  from input multipe spectral images\n","  \n","  Attributes:\n","  data : input data to be encoded\n","  \"\"\"\n","\n","  def __init__(self, in_channel):\n","      super(ConvEncoder,self).__init__()\n","      #Convolution 1\n","      self.conv1=nn.Conv2d(in_channels=in_channel,out_channels=128, kernel_size=7, stride=2, padding = 3)   # padding='valid', there's no padding; padding='same' the input are zero-padded\n","      # nn.init.xavier_uniform(self.conv2.weight)\n","\n","      #Convolution 2\n","      self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, padding='same')\n","      \n","      #Convolution 3\n","      self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same')\n","      self.bn3 = nn.BatchNorm2d(128)\n","\n","      #Convolution 4      \n","      self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, padding='same')\n","      self.bn4 = nn.BatchNorm2d(256)\n","\n","      #Convolution 5      \n","      self.conv5 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, padding='same')\n","      self.bn4 = nn.BatchNorm2d(512)\n","\n","      # activation function\n","      self.relu= nn.ReLU()\n","\n","      #Max Pool 1\n","      self.maxpool = nn.MaxPool2d(kernel_size=2,return_indices=True)\n","\n","      #Average Pool 1\n","      # self.averagepool = nn.AvgPool2d(kernel_size=2,return_indices=True)\n","      # self.maxpool1= nn.MaxPool2d(kernel_size=2,return_indices=True)\n","\n","  def forward(self,x):\n","\n","      pool_info = []\n","\n","      out=self.relu(self.conv1(x))\n","      # out=self.relu(self.conv2(out))\n","      # out=self.bn3(out)\n","\n","      size1 = out.size()\n","      out,indices1=self.maxpool(out)\n","      pool_info.append([indices1,size1])\n","\n","      out=self.relu(self.conv3(out))\n","      out=self.relu(self.conv4(out))\n","      # out=self.bn4(out)\n","      \n","      size2 = out.size()\n","      out,indices2=self.maxpool(out)\n","      pool_info.append([indices2,size2])\n","\n","      # out=self.conv5(out)\n","\n","      return(out, pool_info)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zlMk6eE2sL5t","executionInfo":{"status":"ok","timestamp":1681396820346,"user_tz":300,"elapsed":124,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["\n","class DeConvDecoder(nn.Module):\n","  \"\"\" \n","  reconstruct image from extracted features\n","  \n","  Attributes:\n","  features : input data to be encoded\n","  in_channel: reconstructed channels\n","  \"\"\"\n","  def __init__(self, in_channel):\n","      super(DeConvDecoder,self).__init__()\n","\n","      #De Convolution 1\n","      self.deconv1=nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=1)\n","      # nn.init.xavier_uniform(self.deconv1.weight)\n","      # self.swish4=nn.ReLU()\n","      self.bn1 = nn.BatchNorm2d(512)\n","\n","      #De Convolution 2\n","      self.deconv2=nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=1)\n","      self.bn2 = nn.BatchNorm2d(256)\n","\n","      #De Convolution 3\n","      self.deconv3=nn.ConvTranspose2d(in_channels=128,out_channels=128,kernel_size=3, padding = 1)\n","      self.bn3 = nn.BatchNorm2d(128)\n","\n","      #De Convolution 4\n","      self.deconv4=nn.ConvTranspose2d(in_channels=128,out_channels=128,kernel_size=1)\n","      self.bn4 = nn.BatchNorm2d(128)\n","\n","      #DeConvolution 5\n","      self.deconv5=nn.ConvTranspose2d(in_channels=128,out_channels=in_channel,kernel_size=7, stride=2, padding = 3)\n","      # nn.init.xavier_uniform(self.deconv3.weight)\n","      # self.swish6=nn.ReLU()\n","\n","      # activation function\n","      self.relu= nn.ReLU()\n","\n","      #Max UnPool 1\n","      self.maxunpool=nn.MaxUnpool2d(kernel_size=2)\n","      #Max UnPool 2\n","      # self.maxunpool2=nn.MaxUnpool2d(kernel_size=2)\n","\n","\n","  def forward(self,x, pool_info):\n","\n","      # out=self.relu(self.deconv1(x))\n","\n","      indices2,size2 = pool_info[1]\n","      # out=self.maxunpool(x)\n","      out=self.maxunpool(x,indices2,size2)\n","\n","      out=self.relu(self.deconv2(out))\n","      out=self.relu(self.deconv3(out))\n","      # out=self.bn3(out)\n","\n","      indices1,size1 = pool_info[0]\n","      # out=self.maxunpool(out)\n","      out=self.maxunpool(out,indices1,size1)\n","      \n","      # out=self.relu(self.deconv4(out))\n","      out=self.deconv5(out)\n","      # out=self.swish6(out)\n","\n","      return(out)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"WfDIYROVG3QJ","executionInfo":{"status":"ok","timestamp":1681396823117,"user_tz":300,"elapsed":119,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["class EncoderDecoder(nn.Module):\n","  \"\"\" create convolutional layers to extract features\n","  from input multipe spectral images\n","  \n","  Attributes:\n","  data : input data to be encoded\n","  \"\"\"\n","\n","  def __init__(self, in_channel):\n","      super(EncoderDecoder,self).__init__()\n","      #Convolution 1\n","      self.conv1=nn.Conv2d(in_channels=in_channel,out_channels=128, kernel_size=7, stride=2, padding = 3)   # padding='valid', there's no padding; padding='same' the input are zero-padded\n","      # nn.init.xavier_uniform(self.conv2.weight)\n","      self.relu= nn.ReLU()\n","      #Convolution 2\n","      self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, padding='same')\n","\n","      #Max Pool 1\n","      self.maxpool = nn.MaxPool2d(kernel_size=2,return_indices=True)\n","\n","      #Convolution 3\n","      self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same')\n","\n","      #Convolution 4      \n","      self.conv4 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=1, padding='same')\n","\n","      #Average Pool 1\n","      # self.averagepool = nn.AvgPool2d(kernel_size=2,return_indices=True)\n","      # self.maxpool1= nn.MaxPool2d(kernel_size=2,return_indices=True)\n","\n","      #Convolution 5      \n","      self.conv5 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, padding='same')\n","\n","      \n","      #De Convolution 5\n","      self.deconv5=nn.ConvTranspose2d(in_channels=512, out_channels=512, kernel_size=1)\n","      # nn.init.xavier_uniform(self.deconv1.weight)\n","      # self.swish4=nn.ReLU()\n","      #Max UnPool 1\n","      self.maxunpool=nn.MaxUnpool2d(kernel_size=2)\n","\n","      #De Convolution 4\n","      self.deconv4=nn.ConvTranspose2d(in_channels=512,out_channels=128,kernel_size=1)\n","\n","      #De Convolution 3\n","      self.deconv3=nn.ConvTranspose2d(in_channels=128,out_channels=128,kernel_size=3, padding = 1)\n","\n","      #Max UnPool 2\n","      # self.maxunpool2=nn.MaxUnpool2d(kernel_size=2)\n","\n","      #De Convolution 2\n","      self.deconv2=nn.ConvTranspose2d(in_channels=128,out_channels=128,kernel_size=1)\n","\n","      #DeConvolution 1\n","      self.deconv1=nn.ConvTranspose2d(in_channels=128,out_channels=in_channel,kernel_size=7, stride=2, padding = 3)\n","      # nn.init.xavier_uniform(self.deconv3.weight)\n","      # self.swish6=nn.ReLU()\n","      \n","      \n","  def forward(self,x):\n","      # pool_info = []\n","      out=self.relu(self.conv1(x))\n","      # out,indices1=self.maxpool(out)\n","      out=self.relu(self.conv2(out))\n","      size1 = out.size()\n","      out,indices1=self.maxpool(out)\n","      # pool_info.append([indices1,size1])\n","      # out,indices2=self.maxpool(out)\n","      out=self.relu(self.conv3(out))\n","      out=self.relu(self.conv4(out))\n","      # out,indices2=self.averagepool(out)\n","      size2 = out.size()\n","      out,indices2=self.maxpool(out)\n","      \n","      # pool_info.append([indices2,size2])\n","      # out=self.conv5(out)\n","      # print(pool_info)\n","        # out=self.relu(self.deconv5(x))\n","      # indices2,size2 = pool_info[1]\n","      # out=self.maxunpool(x)\n","      out=self.maxunpool(out,indices2)\n","      out=self.relu(self.deconv4(out))\n","      out=self.relu(self.deconv3(out))\n","      # indices1,size1 = pool_info[0]\n","      # out=self.maxunpool(out)\n","      out=self.maxunpool(out,indices1)\n","      # out=self.maxunpool2(out,indices1,size1)\n","      out=self.relu(self.deconv2(out))\n","      out=self.deconv1(out)\n","      # out=self.swish6(out)\n","      return(out)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1124,"status":"ok","timestamp":1681396828454,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"},"user_tz":300},"id":"jUzQ4KeDWohP","outputId":"6ac5d6d3-3466-412f-b255-6084837af7d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 4, 4])\n","torch.Size([1, 1, 2, 2])\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[[[ 0.,  0.,  0.,  0.],\n","          [ 0.,  6.,  0.,  8.],\n","          [ 0.,  0.,  0.,  0.],\n","          [ 0., 16.,  0., 18.]]]])"]},"metadata":{},"execution_count":14}],"source":["pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n","unpool = nn.MaxUnpool2d(2, stride=2)\n","input = torch.tensor([[[[ 1.,  2.,  3.,  4.],\n","                        [5., 6., 7., 8.,],\n","                        [ 11.,  12.,  13.,  14.],\n","                        [15., 16., 17., 18.]]]])\n","print(input.size())\n","output, indices = pool(input)\n","print(output.size())\n","unpool(output, indices)\n","# # Now using output_size to resolve an ambiguous size for the inverse\n","# input = torch.torch.tensor([[[[ 1.,  2.,  3., 4., 5.],\n","# output, indices = pool(input)\n","# # This call will not work without specifying output_size\n","# unpool(output, indices, output_size=input.size())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1681255321771,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"},"user_tz":300},"id":"WColcaFloCTZ","outputId":"0abf4737-f9f2-457a-e7c8-375707669ca3"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[[ 5,  7],\n","          [13, 15]]]])\n"]}],"source":["print(indices)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBpwXOIo0Gbh"},"outputs":[],"source":["!wget -q  https://share.phys.ethz.ch/~pf/albecker/abc/09072022_1154_train.h5\n","!wget -q  https://share.phys.ethz.ch/~pf/albecker/abc/09072022_1154_val.h5\n","!wget -q https://share.phys.ethz.ch/~pf/albecker/abc/09072022_1154_test.h5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BciJYGHK0OlO"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from abcdataset import get_h5_images\n","from abcdataset import ABCDataset\n","from visualization import ShowBandsCombination, plot_spatial"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"NpdibUel0Q6w","executionInfo":{"status":"ok","timestamp":1681396846112,"user_tz":300,"elapsed":10708,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["\n","# <KeysViewHDF5 ['agbd', 'cloud', 'images', 'lat', 'lon', 'scl']>\n","keys = ['agbd', 'images', 'cloud', 'lat', 'lon', 'scl']\n","data_type = np.float64\n","cat_vi = True\n","cat_cloud = True\n","cat_coord = True\n","cat_scl = True\n","\n","train_file = \"09072022_1154_train.h5\"\n","validate_file = \"09072022_1154_val.h5\"\n","test_file = \"09072022_1154_test.h5\"\n","\n","train_biomasses, train_images = get_h5_images(train_file, keys, data_type, cat_vi, cat_cloud, cat_coord, cat_scl)\n","validate_biomasses,validate_images = get_h5_images(validate_file, keys, data_type, cat_vi, cat_cloud, cat_coord, cat_scl)\n","test_biomasses,test_images = get_h5_images(test_file, keys, data_type, cat_vi, cat_cloud, cat_coord, cat_scl)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8vp4lLbhSEB"},"outputs":[],"source":["max = train_images.max((0,2,3))\n","min = train_images.min((0,2,3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zdxVSVHn-4bo"},"outputs":[],"source":["# showimage = ShowBandsCombination(train_images[0:8,1:11,:], train_biomasses[0:8], [max,0])\n","# showimage.imshow_all_single()\n","# showimage.imshow_selectBands_3bands([2,3,4,5,6,7])\n","# showimage.imshow_selectBands_3bands([[3,2,1],[7,3,2],[7,6,3],[9,8,6]])\n","# imshow_chw(train_images[233,0:11,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kIQk1oJmdUDt"},"outputs":[],"source":["# plot_spatial(train_images[0:2,:], train_biomasses[0:2])"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"KgcG_y9j2358","executionInfo":{"status":"ok","timestamp":1681396856797,"user_tz":300,"elapsed":103,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["validate_biomasses2 = np.log(validate_biomasses)\n","train_biomasses2 = np.maximum(np.log(train_biomasses),0)\n","test_biomasses2 = np.log(test_biomasses)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"GetSX-4b7j0X","executionInfo":{"status":"ok","timestamp":1681396864183,"user_tz":300,"elapsed":111,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["validate_biomasses1 = validate_biomasses/500\n","train_biomasses1 = np.minimum(train_biomasses/500,1)\n","test_biomasses1 = test_biomasses/500"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"PhZtLAXL2qjH","executionInfo":{"status":"ok","timestamp":1681396893653,"user_tz":300,"elapsed":1361,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["train_test = train_images[:,0:15, :,:]\n","validate_test = validate_images[:,0:15,:,:]\n","test_test = test_images[:,0:15,:,:]\n","\n","\n","mean = train_test.mean((0,2,3))\n","std = train_test.std((0,2,3))\n","\n","# Load and preprocess your data as required\n","\n","batch_size = 16\n","\n","train_dataset = ABCDataset(train_test, train_biomasses1, mean, std)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","validate_dataset = ABCDataset(validate_test, validate_biomasses1, mean, std)\n","validate_loader = torch.utils.data.DataLoader(validate_dataset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","test_dataset = ABCDataset(test_test, test_biomasses1, mean, std)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1681396917297,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"},"user_tz":300},"id":"yHnNiJfC1RSY","outputId":"da3bc86a-2d6c-4260-c360-883191d11bb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 128, 8, 8]          94,208\n","              ReLU-2            [-1, 128, 8, 8]               0\n","         MaxPool2d-3  [[-1, 128, 4, 4], [-1, 128, 4, 4]]               0\n","            Conv2d-4            [-1, 128, 4, 4]         147,584\n","              ReLU-5            [-1, 128, 4, 4]               0\n","            Conv2d-6            [-1, 256, 4, 4]          33,024\n","              ReLU-7            [-1, 256, 4, 4]               0\n","         MaxPool2d-8  [[-1, 256, 2, 2], [-1, 256, 2, 2]]               0\n","================================================================\n","Total params: 274,816\n","Trainable params: 274,816\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 39.78\n","Params size (MB): 1.05\n","Estimated Total Size (MB): 40.84\n","----------------------------------------------------------------\n"]}],"source":["from torchsummary import summary\n","\n","in_channel = train_test.shape[1]\n","\n","\n","conv = ConvEncoder(in_channel)\n","summary(conv, (15, 15, 15))\n","deconv = DeConvDecoder(in_channel)\n","# summary(deconv, (512, 2, 2))\n","# conv_deconv = EncoderDecoder(in_channel)\n","# summary(conv_deconv, (10, 15, 15))\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"DXGu2OG8N5eX","executionInfo":{"status":"ok","timestamp":1681396923066,"user_tz":300,"elapsed":109,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["\n","criterion = nn.MSELoss()  # Mean Squared Error loss function\n","optimizer = optim.Adam(list(conv.parameters()) + list(deconv.parameters()), lr=0.001)  # Adam optimizer\n","# optimizer = optim.Adam(conv.parameters().parameters(), lr=0.001)  # Adam optimizer"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"jOZKu7UXTRzV","executionInfo":{"status":"ok","timestamp":1681396927504,"user_tz":300,"elapsed":96,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"outputs":[],"source":["def validate_model(conv, deconv, validate_loader,criterion):\n","    val_loss = 0.0\n","    # evaluate model at end of epoch\n","    for i, data in enumerate(validate_loader, 0):\n","    # Get inputs and labels\n","        inputs, labels = data\n","        \n","        features, pool_size = conv(inputs)\n","        outputs = deconv(features, pool_size )\n","\n","        # Compute loss\n","        loss = criterion(outputs, inputs)\n","        val_loss +=  loss.item()\n","        \n","        return val_loss"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424271,"status":"ok","timestamp":1681401572998,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"},"user_tz":300},"id":"iPa6iM4-2pfq","outputId":"d7ac26c7-4f2e-4d88-f2ed-f1ea82ee6c16"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1,   100] loss: 0.020\n","[1,   200] loss: 0.019\n","[1,   300] loss: 0.020\n","[1,   400] loss: 0.021\n","[1,   500] loss: 0.019\n","[1,   600] loss: 0.020\n","[1,   700] loss: 0.054\n","[1,   800] loss: 0.024\n","[1,   900] loss: 0.034\n","[1,  1000] loss: 0.024\n","[1,  1100] loss: 0.020\n","[1,  1200] loss: 0.021\n","[1,  1300] loss: 0.024\n","[1,  1400] loss: 0.021\n","[1,  1500] loss: 0.020\n","the training loss is 0.3784838331193896, the validation loss is 0.006694563198834658\n","[2,   100] loss: 0.019\n","[2,   200] loss: 0.019\n","[2,   300] loss: 0.022\n","[2,   400] loss: 0.020\n","[2,   500] loss: 0.018\n","[2,   600] loss: 0.029\n","[2,   700] loss: 0.021\n","[2,   800] loss: 0.026\n","[2,   900] loss: 0.021\n","[2,  1000] loss: 0.019\n","[2,  1100] loss: 0.020\n","[2,  1200] loss: 0.021\n","[2,  1300] loss: 0.021\n","[2,  1400] loss: 0.020\n","[2,  1500] loss: 0.019\n","the training loss is 0.3294147147971671, the validation loss is 0.013803680427372456\n","[3,   100] loss: 0.018\n","[3,   200] loss: 0.032\n","[3,   300] loss: 0.031\n","[3,   400] loss: 0.030\n","[3,   500] loss: 0.020\n","[3,   600] loss: 0.019\n","[3,   700] loss: 0.018\n","[3,   800] loss: 0.021\n","[3,   900] loss: 0.021\n","[3,  1000] loss: 0.019\n","[3,  1100] loss: 0.018\n","[3,  1200] loss: 0.019\n","[3,  1300] loss: 0.019\n","[3,  1400] loss: 0.023\n","[3,  1500] loss: 0.019\n","the training loss is 0.3384469295560848, the validation loss is 0.0065752482041716576\n","[4,   100] loss: 0.018\n","[4,   200] loss: 0.019\n","[4,   300] loss: 0.025\n","[4,   400] loss: 0.019\n","[4,   500] loss: 0.018\n","[4,   600] loss: 0.021\n","[4,   700] loss: 0.019\n","[4,   800] loss: 0.019\n","[4,   900] loss: 0.048\n","[4,  1000] loss: 0.023\n","[4,  1100] loss: 0.020\n","[4,  1200] loss: 0.026\n","[4,  1300] loss: 0.020\n","[4,  1400] loss: 0.020\n","[4,  1500] loss: 0.019\n","the training loss is 0.3467819673460326, the validation loss is 0.004956656601279974\n","[5,   100] loss: 0.019\n","[5,   200] loss: 0.018\n","[5,   300] loss: 0.019\n","[5,   400] loss: 0.018\n","[5,   500] loss: 0.016\n","[5,   600] loss: 0.021\n","[5,   700] loss: 0.018\n","[5,   800] loss: 0.027\n","[5,   900] loss: 0.022\n","[5,  1000] loss: 0.019\n","[5,  1100] loss: 0.018\n","[5,  1200] loss: 0.017\n","[5,  1300] loss: 0.017\n","[5,  1400] loss: 0.017\n","[5,  1500] loss: 0.018\n","the training loss is 0.3038695324285072, the validation loss is 0.006726767867803574\n"]}],"source":["\n","num_epochs = 5\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    train_loss = 0.0\n","    val_loss = 0.0\n","    \n","    for i, data in enumerate(train_loader, 0):\n","        # Get inputs and labels\n","        inputs, labels = data\n","        \n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # Forward pass\n","        # \n","        features, pool_size = conv(inputs)\n","        outputs = deconv(features, pool_size )\n","         \n","        # outputs =conv_deconv(inputs)\n","        # Compute loss\n","        loss = criterion(outputs, inputs)\n","        \n","        # Backward pass and optimization\n","        loss.backward()\n","        # update weights\n","        optimizer.step()\n","        \n","        # Print statistics\n","        running_loss += loss.item()\n","        train_loss +=  loss.item()\n","        if i % 100 == 99:    # Print every 100 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss))\n","            running_loss = 0.0\n","\n","    val_loss = validate_model(conv, deconv, validate_loader,criterion)\n","    print(f'the training loss is {train_loss}, the validation loss is {val_loss}')\n","\n","    \n","\n"]},{"cell_type":"code","source":["def plot_spectrums(images, label):\n","  \"\"\"\n","  plot the spectrum for each pixels in the image\n","  image is numpy array with shape (C, H,W)\n","  \"\"\"\n","  # img_shp = images.shape\n","  # n = img_shp[0]\n","  n = len(images)\n","  fig, axs = plt.subplots(1, n, figsize=(n*5, 4))\n","  fig = plt.gcf()\n","  # fig.set_size_inches(18.5,10.5)\n","  fig.suptitle(str(label))\n","\n","  for l in range(n):\n","    image = images[l]\n","\n","    c,h,w = image.shape\n","    img = image.reshape(c,-1)\n","\n","    for i in range(w*h):\n","      axs[l].plot(range(c),img[:c,i])\n","    plt.title(f'The biomass is {label}')\n","    axs[l].set_xlabel('channels')\n","    axs[l].set_ylabel('intensity')\n","    # plt.ylim([0,6000])\n","    # plt.xlim([0,11])\n","\n","  plt.subplots_adjust(right=0.8)\n","  plt.show()\n","  \n","\n","\n","def scatter_spectrums(images, label):\n","  \"\"\"\n","  plot the spectrum for each pixels in the image\n","  image is numpy array with shape (C, H,W)\n","  \"\"\"\n","  # img_shp = images.shape\n","  # n = img_shp[0]\n","  n = len(images)\n","  fig, axs = plt.subplots(1, n, figsize=(n*5, 4))\n","  fig = plt.gcf()\n","  # fig.set_size_inches(18.5,10.5)\n","  fig.suptitle(str(label))\n","\n","  for l in range(n):\n","    image = images[l]\n","\n","    c,h,w = image.shape\n","    img = image.reshape(c,-1)\n","\n","    for i in range(w*h):\n","      axs[l].plot(range(c),img[:c,i], '*')\n","    plt.title(f'The biomass is {label}')\n","    axs[l].set_xlabel('channels')\n","    axs[l].set_ylabel('intensity')\n","    # plt.ylim([0,6000])\n","    # plt.xlim([0,11])\n","\n","  plt.subplots_adjust(right=0.8)\n","  plt.show()\n","  \n","\n","def scatter_spectrum(image, label):\n","  \"\"\"\n","  plot the spectrum for each pixels in the image\n","  image is numpy array with shape (C, H,W)\n","  \"\"\"\n","  c,h,w = image.shape\n","  # # assigning coordinates\n","  # y = np.linspace(1, w*h, w*h)\n","  # x = np.linspace(1, c, c)\n","  # X, Y = np.meshgrid(x, y)\n","  # # reshape image to fit plot\n","  img = image.reshape(c,-1)\n","\n","  # Change the Size of Graph using Figsize\n","  fig = plt.figure(figsize=(6, 5))\n","\n","  for i in range(w*h):\n","    # plt.scatter(range(c),img[:c,i])\n","    plt.plot(range(c),img[:c,i], '*')\n","  plt.title(f'The biomass is {label}')\n","  plt.xlabel('channels')\n","  plt.ylabel('intensity')\n","  # plt.ylim([0,6000])\n","  # plt.xlim([0,11])\n","  plt.show()\n"],"metadata":{"id":"lIwFfutPOeVY","executionInfo":{"status":"ok","timestamp":1681400817109,"user_tz":300,"elapsed":109,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Ptwu3CaRtnBfs-CbeOligQ-n71Zmc418"},"executionInfo":{"elapsed":26358,"status":"ok","timestamp":1681400951577,"user":{"displayName":"Shengfang Ma","userId":"09936596192562256996"},"user_tz":300},"id":"GLy3tTne4HJT","outputId":"1cf26ca0-7f4b-43e0-e520-62b4d09ce4d2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["from visualization import plot_spectrum\n","total_loss = 0\n","total_samples = 0\n","for data in test_loader:\n","    inputs, labels = data\n","    # outputs =conv_deconv(inputs)\n","    features, pool_size = conv(inputs)\n","    outputs = deconv(features, pool_size )\n","      \n","    for l in range(10):\n","\n","        # imgi = inputs[l].permute(1,2,0) \n","        imgi = inputs[l].detach().numpy()\n","        imgi_ndvi = imgi[12:14,:]\n","        imgi_lswi = imgi[14,:]\n","        imgi = imgi[0:12,:]\n","        # print(imgi.size())\n","\n","        # imgo = outputs[l].permute(1,2,0)\n","        # print(imgo.size())\n","        #imgi = imgi[:,:,2:5]\n","        imgo = outputs[l].detach().numpy()\n","        imgo_ndvi = imgo[12:14,:]\n","        imgo_lswi = imgo[14,:]\n","        imgo = imgo[0:12,:]\n","\n","        img_diff = imgo - imgi\n","        error = np.sum(np.power(img_diff,2))\n","\n","        img_diff = img_diff/imgi\n","        # img_diff =  img_diff / img_diff.max()\n","\n","        scatter_spectrums([imgi,imgo],labels[l]*500)\n","        # plot_spectrum(imgo,labels[l]*500)\n","\n","        scatter_spectrums([imgi_ndvi,imgo_ndvi],labels[l]*500)\n","        # plot_spectrum(imgo_ndvi,labels[l]*500)\n","        scatter_spectrum(img_diff,labels[l]*500)\n","        print(error)\n","\n","        # fig, axs = plt.subplots(1,3, figsize=(10, 6))\n","        # fig = plt.gcf()\n","        # fig.set_size_inches(18.5,10.5)\n","       \n","        # img_show = np.array(imgi[:,:,0:3])\n","        # # m = img_show.max()\n","        # # img_show = img_show / m\n","        \n","        # img_show2 = np.array(imgo[:,:,0:3].detach().numpy())\n","        # # m = img_show2.max()\n","        # # img_show2 = img_show2 / m\n","\n","        # axs[0].imshow(img_show/img_show.max())\n","        # axs[1].imshow(img_show2/img_show.max())\n","        # img_diff = (img_show2 - img_show)\n","        # axs[2].imshow(img_diff/img_diff.max())\n","        # error = np.sum(np.power(img_diff,2))\n","        # a = img_diff/img_show\n","        # axs[3].imshow(a)\n","\n","        # print(error)\n","        # # axs[2].title(str(error))\n","        # plt.show()\n","\n","\n","      \n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sf31kCVzLAlk"},"outputs":[],"source":["from visualization import plot_spectrum\n","total_loss = 0\n","total_samples = 0\n","for data in test_loader:\n","    inputs, labels = data\n","    # outputs =conv_deconv(inputs)\n","    features, pool_size = conv(inputs)\n","    outputs = deconv(features, pool_size )\n","      \n","    for l in range(5):\n","\n","        # imgi = inputs[l].permute(1,2,0) \n","        imgi = inputs[l].detach().numpy()\n","        # print(imgi.size())\n","\n","        # imgo = outputs[l].permute(1,2,0)\n","        # print(imgo.size())\n","        #imgi = imgi[:,:,2:5]\n","        imgo = outputs[l].detach().numpy()\n","\n","        plot_spectrum(imgi,labels[l]*500)\n","        plot_spectrum(imgo,labels[l]*500)\n","\n","        img_diff = imgo - imgi\n","        error = np.sum(np.power(img_diff,2))\n","\n","        img_diff = img_diff/imgi\n","        # img_diff =  img_diff / img_diff.max()\n","\n","        plot_spectrum(img_diff,labels[l]*500)\n","        print(error)\n","\n","        # fig, axs = plt.subplots(1,3, figsize=(10, 6))\n","        # fig = plt.gcf()\n","        # fig.set_size_inches(18.5,10.5)\n","       \n","        # img_show = np.array(imgi[:,:,0:3])\n","        # # m = img_show.max()\n","        # # img_show = img_show / m\n","        \n","        # img_show2 = np.array(imgo[:,:,0:3].detach().numpy())\n","        # # m = img_show2.max()\n","        # # img_show2 = img_show2 / m\n","\n","        # axs[0].imshow(img_show/img_show.max())\n","        # axs[1].imshow(img_show2/img_show.max())\n","        # img_diff = (img_show2 - img_show)\n","        # axs[2].imshow(img_diff/img_diff.max())\n","        # error = np.sum(np.power(img_diff,2))\n","        # a = img_diff/img_show\n","        # axs[3].imshow(a)\n","\n","        # print(error)\n","        # # axs[2].title(str(error))\n","        # plt.show()\n","\n","\n","      \n","    break"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHQS6Z+jHmjA5PZadvqGxu"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
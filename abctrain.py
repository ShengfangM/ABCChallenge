# -*- coding: utf-8 -*-
"""abctrain.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQS5klYhGTAVBisii1RU71vXdVe7zjgK
"""

import torch
import torch.nn as nn
# import torch.nn.functional as F
# # import torchvision.transforms as transforms
# # import torchvision.models as models
# import torch.optim as optim
# import matplotlib.pyplot as plt
# import numpy as np


def evaluate_conv_deconv(conv, deconv, validate_loader,criterion):
    val_loss = 0.0
    # evaluate model at end of epoch
    for i, data in enumerate(validate_loader, 0):
    # Get inputs and labels
        inputs, labels = data
        
        features, pool_size = conv(inputs)
        outputs = deconv(features, pool_size )

        # Compute loss
        loss = criterion(outputs, inputs)
        val_loss +=  loss.item()
        
        return val_loss


# Evaluate the model
def evaluate_predict(predict_model, head_model, validate_loader,criterion, factor):
    with torch.no_grad():
        total_loss = 0
        total_samples = 0
        for data in validate_loader:
            inputs, labels = data

            features, pool_size = head_model(inputs)
            outputs = predict_model(features)
            # loss = criterion(torch.exp(outputs), torch.exp(labels))
            loss = criterion(outputs*factor, labels*factor)
            total_loss += loss.item() * inputs.size(0)
            total_samples += inputs.size(0)
        avg_loss = total_loss / total_samples
        print('Average test loss: %.3f' % avg_loss)
    return total_loss


def evaluate_model(model, validate_loader,criterion, factor):
  # Evaluate the model
  with torch.no_grad():
      total_loss = 0
      total_samples = 0
      for data in validate_loader:
          inputs, labels = data
          outputs = model(inputs)

          loss = criterion(outputs*factor, labels*factor)

          # loss = criterion(torch.exp(outputs), torch.exp(labels))
          total_loss += loss.item() * inputs.size(0)
          total_samples += inputs.size(0)
      avg_loss = total_loss / total_samples
      print('Average test loss: %.3f' % avg_loss)
  return total_loss


def train_conv_deconv(conv_model, deconv_model, train_loader, validate_loader, criterion, optimizer, num_epochs):

    num_epochs = 5
    for epoch in range(num_epochs):
        running_loss = 0.0
        train_loss = 0.0
        
        for i, data in enumerate(train_loader, 0):
            # Get inputs and labels
            inputs, labels = data
            
            # Zero the parameter gradients
            optimizer.zero_grad()
            
            # Forward pass
            # 
            features, pool_size = conv_model(inputs)
            outputs = deconv_model(features, pool_size )
            # outputs =conv_deconv(inputs)

            # Compute loss
            loss = criterion(outputs, inputs)
            
            # Backward pass and optimization
            loss.backward()
            # update weights
            optimizer.step()
            
            # Print statistics
            running_loss += loss.item()
            if i % 100 == 99:    # Print every 100 mini-batches
                print('[%d, %5d] loss: %.3f' %
                      (epoch + 1, i + 1, running_loss))
                train_loss += running_loss
                running_loss = 0.0

        val_loss = evaluate_conv_deconv(conv_model, deconv_model, validate_loader,criterion)
        print(f'the training loss is {train_loss}, the validation loss is {val_loss}')

    # # save each
    #     torch.save({
    #             'epoch': epoch,
    #             'model_state_dict': conv_model.state_dict(),
    #             'optimizer_state_dict': optimizer.state_dict(),
    #             'loss': val_loss,
    #             }, f'/path/conv_statedict_epoch_{epoch}_loss_{val_loss}')
        # save the whole model
        # torch.save(conv_model, f'/path/Conv_{epoch}_loss_{val_loss}')



def train_predict(head_model, predict_model,  criterion, optimizer, train_loader, validate_loader, num_epochs, factor):

  for epoch in range(num_epochs):
      running_loss = 0.0
      train_loss = 0.0
      
      for i, data in enumerate(train_loader, 0):
          # Get inputs and labels
          inputs, labels = data
          
          # Zero the parameter gradients
          optimizer.zero_grad()
          
          # Forward pass
          features, pool_size = head_model(inputs)
          outputs = predict_model(features)
          
          # Compute loss
          loss = criterion(outputs, labels)
          # Backward pass and optimization
          loss.backward()
          # update weights
          optimizer.step()
          
          # Print statistics
          running_loss += loss.item()
          if i % 100 == 99:    # Print every 100 mini-batches
              print('[%d, %5d] loss: %.3f' %
                    (epoch + 1, i + 1, running_loss))
              train_loss +=running_loss
              running_loss = 0.0

      # validate the model
      val_loss = evaluate_predict(predict_model, head_model, validate_loader,criterion, factor)
      print(f'the training loss is {train_loss}, the validation loss is {val_loss}')

      # # save each
	  # torch.save(predict_model.state_dict(),f'/path/predict_epoch_{epoch}_loss_{val_loss}.pth')
      # torch.save({
      #         'epoch': epoch,
      #         'model_state_dict': predict_model.state_dict(),
      #         'optimizer_state_dict': optimizer.state_dict(),
      #         'loss': val_loss,
      #         }, f'/path/predict_epoch_{epoch}_loss_{val_loss}')
      # # save the whole model
      # torch.save(predict_model, f'/path/Predict_model_epoch_{epoch}_loss_{val_loss}')



def train_model(model,  criterion, optimizer, train_loader, validate_loader, num_epochs, factor):

    # net = net.float()
    num_epochs = 5
    for epoch in range(num_epochs):  # loop over the dataset multiple times
        train_loss = 0.0
        running_loss = 0.0
        for i, data in enumerate(train_loader, 0):
            # get the inputs; data is a list of [inputs, labels]
            inputs, labels = data

            # print(inputs.dtype)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            # print statistics
            # running_loss += loss.item()
            running_loss = loss.item()
            if i % 100 == 0:    # print every 2000 mini-batches
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.3f}')
                print(outputs.flatten())
                print(labels.flatten())
                train_loss += running_loss
                running_loss = 0.0

        val_loss = evaluate_model(model, validate_loader,criterion, factor)
        print(f'the training loss is {train_loss}, the validation loss is {val_loss}')

        # # save each
        # torch.save({
        #       'epoch': epoch,
        #       'model_state_dict': model.state_dict(),
        #       'optimizer_state_dict': optimizer.state_dict(),
        #       'loss': val_loss,
        #       }, f'/path/predict_epoch_{epoch}_loss_{val_loss}')
        # # save the whole model
        # torch.save(predict_model, f'/path/Predict_model_epoch_{epoch}_loss_{val_loss}')